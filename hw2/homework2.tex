\documentclass[]{article}

%opening
\title{FML Fall 2015 HW 2}
\author{David Kasofsky}

\usepackage{amsmath}

\usepackage{amssymb}

\usepackage{fancyhdr}

\pagestyle{fancy}
\lhead{David Kasofsky - FML Fall 2015 HW 2}

\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thesubsubsection}{\Alph{subsubsection}}

\begin{document}

\section{}
\subsection{}
As in the related homework problem, we can think of this as a neural network with one input node, $T$ internal nodes, and one output node. At each internal node, we may select a function from $H$ and so the growth function is $\Pi_H(m)$. At the output node we have some growth function $\Pi_O(m)$. Thus the overall growth function is $\Pi_H(m)^T\Pi_O(m)$ as there are $T$ internal nodes leading to the single output node. We can use Sauer's Lemma to bound the growth function in terms of the VC-dimension. Note the the output node defines a $T$-dimensional hyperplane and so has VC-dimension $T+1$. So:\\

$\Pi_O(m) \le (\frac{e m}{T+1})^{T+1}$\\

and:\\ 

$\Pi_H(m)^T \le (\frac{e m}{d})^{dT} $\\

where $d$ is the VC-dimension of $H$. Continuing we have:\\

$\Pi_H(m)^T\Pi_O(m) \le (\frac{e m}{T+1})^{T+1} (\frac{e m}{d})^{dT}$\\

$= (e m)^{dT+T+1} (\frac{1}{T+1})^{T+1} (\frac{1}{d})^{dT} \le (e m)^{dT+T+1}$\\

Now, let $m = 2(dT+T+1)log_2((dT+T+1)e)$. By the inequality in the hint of the related homework problem and $T \ge 1 \implies (dT+T+1)e > 4$, we have:\\

$m \ge (dT+T+1) log_2(e m) \implies 2^m \ge (e m)^{(dT+T+1)}$\\

And so the VC-dimension of $H$ is less than
\begin{center}
	$2(dT+T+1)log_2((dT+T+1)e)$
\end{center}

\pagebreak

\section{}
\subsection{}

$(\implies)$ Assume $\lbrace X^+ \cup \lbrace x_{m+1} \rbrace, X^- \rbrace$ and $\lbrace X^+, X^- \cup \lbrace x_{m+1}\rbrace \rbrace$ are each linearly separable by hyperplanes through the origin. Let $w^+$ and $w^-$ be vectors defining such hyperplanes. We may then take a convex combination $\alpha  w^+ + (1-\alpha) w^- = w^*$ with $w^* \cdot x_{m+1} = 0$. Because the hyperplanes defined by both $w^+$ and $w^-$ linearly separate $\lbrace X^+, X^- \rbrace$, so does one generated by a convex combination. Thus $\lbrace X^+, X^- \rbrace$ is linearly separated by the hyperplane defined by $w^*$, a hyperplane through $x_{m+1}$ and the origin.\\

\noindent$(\impliedby)$ Assume $\lbrace X^+, X^- \rbrace$ is linearly separable by a hyperplane defined by some vector $w$ through the origin and $x_{m+1}$. Let $w^*$ be a vector such that $w^* \cdot x_{m+1} = 1$. Now we can choose some $\epsilon > 0$ so that $\lbrace X^+, X^- \rbrace$ is linearly separable by $w + \epsilon w^*$. However note that now $\lbrace X^+ \cup \lbrace x_{m+1} \rbrace, X^- \rbrace$ is linearly separably by $w + \epsilon^+ w^*$ and, by symmetry, $\lbrace X^+, X^- \cup \lbrace x_{m+1}\rbrace \rbrace$ by $w - \epsilon^- w^*$.
\subsection{}

Consider $C(m+1,d)=C(m,d) + C(m,d-1)$. $C(m,d)$ is the number of linearly separable labelings (LSLs) of $m$ points. Now consider a new point $x_{m+1}$. It is still possible to generate $C(m,d)$ LSLs of the $m+1$ points. Let $L$ be some LSL of the original $m$ points. If it is possible to generate $L$ using a hyperplane through $x_{m+1}$, then we know we can label $x_{m+1}$ either positively or negatively by the result of the previous question. If we cannot generate $L$ with a hyperplane through $x_{m+1}$, then any hyperplane that generates $L$ will label $x_{m+1}$ in some way. This shows that $C(m+1,d) \ge C(m,d)$.

To show that $x_{m+1}$ gives rise to exactly $C(m,d-1)$ additional LSLs, consider the case in which $L$ can be generated by a hyperplane through $x_{m+1}$. Then we may project the $m$ points onto a $d-1$ dimensional hyperplane perpendicular to $x_{m+1}$, retaining both the dichotomy $L$ and the property than any $d-1$ points in this lower dimensional space are also linearly independent. To see this, first assume that any $d-1$ projected points are not linearly independent. Let\\

$P = \left[
\begin{array}{c}
\Leftarrow p_1 \Rightarrow\\
.\\
.\\
.\\
\Leftarrow p_{d-1} \Rightarrow\\
\end{array}
\right]
$\\

be the associated projection matrix, $\alpha_i \in \mathbb{R}$, and $v_i \in X$. Then we would have:\\

$\sum_{i=1}^{d-1} \alpha_i P v_i = 0\\
$
$
\implies \left[
\begin{array}{c}
\left[\sum_{i=1}^{d-1} \alpha_i v_i\right]  \cdot p_1\\
.\\
.\\
.\\
\left[\sum_{i=1}^{d-1} \alpha_i v_i\right]  \cdot p_{d-1}
\end{array}
\right]
=0\\\\\\
$
$
\implies
\begin{array}{c}
\left[\sum_{i=1}^{d-1} \alpha_i v_i\right]  \cdot p_1 = 0\\
.\\
.\\
.\\
\left[\sum_{i=1}^{d-1} \alpha_i v_i\right]  \cdot p_{d-1} = 0
\end{array}
$

However each of these dot products $\left[\sum_{i=1}^{d-1} \alpha_i v_i\right]  \cdot p_j$ is only zero when the vectors are perpendicular. This implies there is a linear combination of $v_1, ..., v_{d-1}$ that is perpendicular to each of $p_1, ..., p_{d-1}$ and hence there is linear combination a of $v_1, ..., v_{d-1}$ equal to $x_{m+1}$. This is because $p_1, ..., p_{d-1}$ form a basis for a hyperplane perpendicular to $x_{m+1}$. However this cannot be the case since this implies that $\lbrace v_1, ..., v_{d-1}, x_{m+1} \rbrace $ are not linearly independent, which contradicts our original assumption that any $d$-subset of $X$ is linearly independent.

To see that the $L$ is maintained, let $p_1 = w$ where $w$ is the vector defining the hyperplane through $x_{m+1}$ that generates $L$ and $p_2, ..., p_{d-1}$ be orthogonal to $w$ to form a basis for the $d-1$ dimensional subspace. Then the sign of a point in the lower-dimensional space is:\\

$sign(Pw \cdot Pv) = 
sign(
\left[
\begin{array}{c}
w \cdot w \\
w \cdot p_2\\
.\\
.\\
w \cdot p_{d-1}\\
\end{array}
\right]
\cdot
\left[
\begin{array}{c}
v \cdot w \\
v \cdot p_2\\
.\\
.\\
v \cdot p_{d-1}\\
\end{array}\\
\right]
)\\
=
sign(
\left[
\begin{array}{c}
w \cdot w \\
0\\
0\\
0\\
0
\end{array}
\right]
\cdot
\left[
\begin{array}{c}
v \cdot w \\
v \cdot p_2\\
.\\
.\\
v \cdot p_{d-1}\\
\end{array}
\right])\\
= sign(||w||^2 v \cdot w)
$\\
which is the same sign given by $L$. We could do this in each case $L$ goes through $x_{m+1}$ but all subspaces obtained this way would be a rotations of each other, so it suffices to project onto a single subspace using projection matrix $P^*$. This is ok because rotation preserves the cosine similarity of vectors and therefore their signs with respect to a hyperplane. Thus for every $L$ which can be generated with a hyperplane through $x_{m+1}$, there is an LSL of the $m$ points in $d-1$ dimensions.

Conversely, consider an LSL $L_{w_{P^*}}$ of the $m$ points in $d-1$ dimensions generated by a hyperplane defined by some vector $w_{P^*}$. Now we may choose a vector $w$ such that $P^* w = w_{P^*}$ and $w \cdot x_{m+1} = 0$. This is because $w$ has $d$ degrees of freedom but is constrained in only $d-1$ by $w_{P^*}$. As above, we know that $sign(w \cdot v) = sign(w_{P^*} \cdot P^*v)$ and so $L_{P^*} = L_{w}$. Thus for every LSL in the $d-1$ dimensional space, there is an LSL in the $d$ dimensional space generated by a hyperplane through $x_{m+1}$. 

This and the last part show that number of additional labelings due to the point $x_{m+1}$ is equal to $C(m, d-1)$ and so the total number of LSLs $C(m+1,d)=C(m,d)+C(m,d-1)$.

To show that formula $C(m,d) = 2\sum_{k=0}^{d-1}\binom{m-1}{k}$ holds, we consider the base case with $m=1$ and $d=2$. Obviously there are only 2 possible LSLs when $m=1$. Additionally the formula yields $C(1,2) = 2\sum_{k=0}^{1}\binom{0}{k}= 2(1 + 0) = 2$. Now for the induction we find:\\\\
$
C(m,d) + C(m, d-1) = 2\left[\sum_{k=0}^{d-1}\binom{m-1}{k}+\sum_{k=0}^{d-2}\binom{m-1}{k}\right]\\\\
=2\sum_{k=0}^{d-1}\binom{m}{k} = C(m+1,d)
$\\

\noindent because of the identity $\binom{m}{k} = \binom{m-1}{k} + \binom{m-1}{k-1}$.

%%Need to fill in combinatorial this but easy check out https://en.wikipedia.org/wiki/Binomial_coefficient#Recursive_formula
%%works term by term
\subsection{}
For any function in $\mathcal{F}$, there is a corresponding vector defining a hyperplane through the origin. Thus $\Pi_{\mathcal{F}}(m)$ is equal to the number of linearly separable labelings of $m$ points in $p$ dimensions, i.e. $C(m,p)$. By the result of the previous question, we then have directly that $\Pi_{\mathcal{F}}(m)$, is $C(m,p) = 2\sum_{i=0}^{p-1}\binom{m-1}{i}$.
\pagebreak

\section{}

\pagebreak

\section{}

\subsection{}

First, note that:\\ $\cos(x_i^2-y_i^2) = \cos x_i^2 \cos y_i^2 + \sin x_i^2 \sin y_i^2 
= \left[
\begin{array}{c}
\cos x_i^2\\
\sin x_i^2\\
\end{array}
\right]
\cdot
\left[
\begin{array}{c}
\cos y_i^2\\
\sin y_i^2\\
\end{array}
\right]$\\

Thus $\cos(x_i^2-y_i^2)$ is a PDS kernel by explicit representation as an inner product. This holds due to Mercer's Condition. Therefore, $\forall n \in \mathbb{N}$:\\

 $\cos^n(x_i^2-y_i^2)$\\
 
 \noindent is a PDS kernel by the product closure property of PDS kernels, as is:\\
 
 $\sum_{i}^{N} \cos^n(x_i^2-y_i^2)$\\
 
 \noindent by the sum closure property.
 
 \subsection{}
 
 By example 5.4 in the book, we know $||x-y||^2$ is an NDS kernel. By theorem 5.7, we have that $e^{-t||x-y||^2}$ is a PDS kernel for positive $t$ and hence $-e^{-t||x-y||^2}$ is NDS.

Now consider $1-e^{-t||x-y||^2}$. Since $-e^{-t||x-y||^2}$ is NDS, we know that:\\

$\sum_{i,j=1}^{m}c_i c_j (-e^{-t||x_i - x_j||^2}) \le 0$\\

\noindent for $c$ such that $1^t c = 0$. But then:\\

$\sum_{i,j=1}^{m}c_i c_j (1-e^t||x_i - x_j||^2) = \sum_{i,j=1}^{m} c_i c_j + \sum_{i,j=1}^{m}c_i c_j (-e^t||x_i - x_j||^2)$\\\\
$= \sum_{i,j=1}^{m}c_i c_j (-e^t||x_i - x_j||^2) \le 0.$\\

\noindent $\sum_{i,j=1}^{m} c_i c_j = 0$ because of the constraint on $c$. Thus $1-e^{-t||x-y||^2}$ is NDS for positive $t$, as is $\frac{1-e^{-t||x-y||^2}}{t^{\frac{3}{2}}}$. Thus the integral over the that expression:\\

$\frac{1}{2 \Gamma(\frac{1}{2})}\int_{0}^{\infty +} \frac{1-e^{-t||x-y||^2}}{t^{\frac{3}{2}}}dt$\\ 

\noindent is NDS as well, since $\frac{1}{2 \Gamma(\frac{1}{2})}$ is a positive constant and

\noindent $\sum_{i,j=1}^{m}c_i c_j (1-e^t||x_i - x_j||^2) \le 0 \implies \int_{0}^{\infty +} \sum_{i,j=1}^{m}c_i c_j (1-e^t||x_i - x_j||^2) dt \le 0$.\\

\noindent The hint tells us this integral is equal to $||x-y||$ and so $||x-y||$ is also NDS. Finally, once again by theorem 5.7, we have that $e^{-\frac{||x-y||}{\sigma}}$ is PDS for any $\sigma > 0$. 

\end{document}